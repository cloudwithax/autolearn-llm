# FP8 GRPO Configuration for 8x RTX 5090 (256GB total VRAM)
# Target: Complete training in ~30-45 minutes
#
# RTX 5090 specs: 32GB GDDR7, native FP8, Blackwell architecture
# 8 cards = 256GB VRAM, massive parallelism

model:
  # 70B fits across 8x 5090s with FP8
  name: "unsloth/Qwen3-32B"
  # Alternatives:
  # - unsloth/Qwen3-32B (fits on 2 GPUs, faster)
  # - unsloth/Qwen3-14B (fits on 1 GPU, fastest)
  # - deepseek-ai/DeepSeek-Coder-V2-Instruct (236B MoE, needs all 8)

  max_seq_length: 8192
  load_in_4bit: false
  load_in_fp8: true # Native FP8 on Blackwell
  fast_inference: true # vLLM

lora:
  rank: 128
  alpha: 128
  dropout: 0.0
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj

training:
  # Max generations - 8 GPUs can handle it
  num_generations: 16
  max_new_tokens: 1024

  learning_rate: 2.0e-5 # Higher LR with large effective batch
  num_train_epochs: 1

  # Large batch per GPU, 8 GPUs
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 2
  # Effective batch = 4 * 2 * 8 = 64

  warmup_ratio: 0.03
  weight_decay: 0.01
  gradient_checkpointing: true

grpo:
  beta: 0.04
  reward_weights:
    test_pass: 0.45
    execution: 0.25
    syntax: 0.15
    lint: 0.10
    complexity: 0.05

dataset:
  name: "humaneval"
  max_samples: 164 # Full HumanEval

execution:
  timeout: 10.0
  max_memory: 512
  allow_network: false

output:
  dir: "./outputs/code_8x5090"
  save_steps: 25
  logging_steps: 5
  push_to_hub: false

# =============================================================================
# ACCELERATION - MAXED FOR 8x 5090
# =============================================================================
acceleration:
  async_rewards:
    enabled: true
    workers: 32 # Tons of CPU cores on rental machines

  reward_cache:
    enabled: true
    max_size: 100000

  curriculum:
    enabled: false # Skip for speed (1 epoch)

  speculative:
    enabled: true
    num_tokens: 8
    dynamic: true

  token_packing:
    enabled: true
    min_pack_ratio: 0.85

  mode: "aggressive"

# =============================================================================
# MULTI-GPU CONFIGURATION
# =============================================================================
distributed:
  # 8x 5090 setup
  num_gpus: 8

  # For 70B model: tensor parallel across 4 GPUs, data parallel 2x
  tensor_parallel_size: 4
  data_parallel_size: 2

  # FSDP config
  fsdp:
    enabled: true
    sharding_strategy: "FULL_SHARD" # Max memory efficiency
    cpu_offload: false # Not needed with 256GB
    backward_prefetch: "BACKWARD_PRE"

  # vLLM multi-GPU generation
  vllm:
    tensor_parallel_size: 4 # Match model TP
    gpu_memory_utilization: 0.92 # Leave headroom
    max_num_seqs: 256 # High throughput
